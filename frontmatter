The Hardest AI Problem and the Metacognitive Loop 

Our long-range aims

It is widely agreed that AI has been extremely successful in the
narrow: given any of a large variety of well-deﬁned AI problems, there 
is (or, so goes the common wisdom, there could easily be) an
implemented solution using reasonably well-understood techniques. Or
at the very least, there are solutions to similar problems, so that
there is much reason to expect solutions to the problem at hand.  

But in the wide sense, AI is a disappointment. Although many AI
systems are at or beyond human-level competence at the individual
tasks for which they were designed, AI systems are nowhere near
human-level competence across the board. This is not simply a matter
of building a system that has numerous subsystems, one for each of
hundreds or thousands of individual tasks. The combined skills of a
doctor, lawyer, chess-master, acrobat, linguist, etc, does not a human
make. For humans have, in addition to whatever else, so-called common
sense, the ability to do reasonably well when faced with surprise,
with situations we are not trained in or familiar with. Indeed, this
ability may well be key to how we are able to become trained: we
recognize when we lack an ability, we make a decision to gain it, and
we recognize when it is time to stop – e.g., when training is
complete, or complete enough for our purposes; or when it is
ineffctive, too slow, too expensive, or no longer important, etc. We
also (sometimes!) recognize when we are in over our heads, and then we
ask for help or wisely give up.

Thus commonsense is not, in our view, likely to be achievable by
microworld approaches, nor more generally by any approaches that
require a fixed KR...

We learn kinds of failures and kinds of responses.

If worse comes to worst, an agent can simply decide it's in over its
head, and either ask for help or slow down and see what it can learn,
or both, or give up altogether. The learning abilities then will play
a huge role.

