.th
.sc
.EQ
delim $$
.EN
.ls 1
.ce 100
\fBSTEP-LOGICS: AN ALTERNATIVE APPROACH 
TO LIMITED REASONING\fR
.sp 
Jennifer Drapkin and Donald Perlis
.sp
University  of  Maryland
Department of Computer Science
College Park, Maryland 20742 USA
(301) 454-2002
.sp
.ce 0
.ls 1
Abstract: We propose that a new kind of logical study is appropriate to
agents engaged in commonsense reasoning, namely, one that focuses on the
steps of reasoning at any given time
rather than the collection of all conclusions ever
reached.
.sp 2
.ls 2
\fBI. Introduction\fR
.pp
The idea that commonsense reasoning is necessarily resource-bound, and in
particular is not closed under ordinary logical consequence, has been
suggested frequently. 
The kind of resource limitation that is most evident in commonsense
reasoning (i.e., in reasoning about and within a real environment) is
simply the very fact of passage of time while the reasoner reasons. There is
not necessarily (or even likely) any fixed and final set of consequences
that such a reasoning agent ends up with. In fact, the paradigm for such an
agent would seem to be that suggested by Nilsson [1983], namely, a computer
individual with a lifetime of its own. What is of interest for such an agent
is not then its ``ultimate'' set of conclusions, but rather its changing set
of conclusions over time. Indeed, there will be, in general, no ultimate or
limiting set of conclusions.
.pp
The ``passage of time'' phenomenon is a limitation only in the following sense: the conclusions
that may be logically (or otherwise) entailed by the agent's information (beliefs) take
time to be derived, and time spent in such derivations is concurrent with
changes in the world. Thus, it is not appropriate
to spend hours figuring out a plan
to save Nell from an onrushing train; she will no longer need saving by then
(see [McDermott 1982] and [Haas 1985]). Even if the only changes are within 
the agent,
this is still important, for it may be useful to know whether a problem is
nearing solution, or if one has only begun initial explorations, and so on.
That is, the agent should be able to reason about its ongoing reasoning
efforts themselves. Thus, it is not so much an issue of weak resources as it
is of a real-world fact about processes occurring over time. In fact,
implemented reasoning systems obviously proceed to draw conclusions in steps;
see for instance [Drapkin&Miller&Perlis 1986, Perlis 1981, Perlis 1984].
.pp
The literature contains a number of approaches to limited reasoning,
apparently with these issues as motivational guides. 
However, most alternative approaches do not, in our
view, carry this out to its logical (no pun intended) conclusions. 
With very
little exception, the oversimplification of a ``final'' state of reasoning
is maintained, and the limitation amounts to a reduced set of consequences
rather than an ever-changing set of tentative conclusions. Thus, Konolige [1985a],
for instance, studies agents with fairly arbitrary rules of inference, but
assumes logical closure for the agents with respect to those rules, ignoring
the effort involved in performing the deductions. Similarly, Levesque [1984] and
Fagin&Halpern [1985] provide formal treatments of limited reasoning, so that,
for instance, a contradiction may go unnoticed; but all the conclusions that
\fIare\fR drawn are done so instantaneously, i.e., the steps of reasoning
involved are not explicit. Lakemeyer [1986] deals with
issues raised by then adding quantifiers, but does not address the issue
that concerns us here. Vardi [1986] also deals with limitations
on omniscience, but again without taking steps into account.
.pp
It is easy to provide examples in which the effort or time spent is crucial.
The example of Nell is one illustration. For another, consider two agents, A
and B, each of which has the intellectual ability (inference rules, etc.) to
derive conclusion C. The two agents are told to try to determine whether C
is true. But each derives that the other can derive C, and so each relaxes
in the (mistaken) assumption that the other already must have derived C.
.pp
In part, this is a problem of modelling time, as has been studied by Allen
[1984] and McDermott [1982]. However, there is more to it than this. Not only must
the agent be able to reason about time, it must be able to reason \fIin\fR
time. That is, as it makes more deductions, time passes, and this fact
itself must be recognized. Otherwise we again face the prospect of losing
Nell while deducing that it will take too long to get to a phone to call the
train depot. We may even take too long to deduce that it will take too long! In
other words, even the treatments of time in the literature are themselves
still in the standard mold of unlimited or instantaneous reasoning.
.br
\fBII. Two languages for steps\fR
.pp
This distinction leads to two directions for study.  First, one would like an
analytic formalism (AF) allowing us to determine what a given agent has and
has not done at any given time.  Second, the \fIagent\fR should also be able to
reason (in some language/structure formalism RF) about what it has and has
not done at any given time.  These are obviously interrelated, and yet can be
tackled somewhat independently.  In particular, the ``analytic
completeness'' of AF can be tackled without requiring the same of the agent,
and \fIvice versa\fR.  That is, we can seek a theory AF with the property
that, for any given time i, and for any given wff \(*a in the agent's language,
AF should allow either a proof that the agent knows (has proved or otherwise
determined) \(*a at i, or a proof that the agent has not done so.
.pp
Here we propose some tentative characteristics of such languages as outlined
above, including  a precise characterization of analytic
completeness. In [Drapkin&Perlis 1986] we have initiated a study with this
aim in mind.
.br
\fBIII. Step-Logics\fR
.pp
We propose three major mechanisms
to study as additions to RF:
self (S), time (T), and retraction (R). 
Since it is important for the
agent to reason about his own processes, a self, or
belief, operator is needed.  The agent would then have a way to
talk (think) about his beliefs.  In order for the agent 
to reason about time, a time operator is needed.
Finally, since we want to be able to deal with commonsense reasoning, the
agent will have to use default reasoning.
That is,
a particular fact is believed if there is no evidence to the 
contrary; however, later, in the face of new evidence, the 
former belief may
be retracted.  For this, we will need some
kind of a retraction device.  
These new operators, of course, will necessitate
corresponding changes in AF if the latter is to be able to ``keep up'' with
a complete analysis of RF.
.pp
For the purposes of study, we have chosen
to add these operators individually at first.
We therefore propose a sequence of step-logics.  
We have come up with the following list.
All the logics include time steps for AF.
The list can be thought of as a progression or lattice in which
later versions incorporate more features into RF.
In $SL sub 0$, RF corresponds to a step-like propositional logic; later we
illustrate this with a sample axiom.
.br
.in +5
.nf
$SL sub 0$ 
$SL sub 1$:  S 
$SL sub 2$:  T 
$SL sub 3$:  R 
$SL sub 4$:  S, R 
$SL sub 5$:  S, T 
$SL sub 6$:  T, R 
$SL sub 7$:  S, T, R 
.fi
.in -5
.pp
It is again important to note the distinction between the \fIagent's\fR language
and theory (RF), and \fIour\fR (the scientist's) language and theory (AF).
The agent has one set of symbols, axioms, and rules, while we have another.
For instance, ``$"" sup i \(*a$'' is used to indicate that the agent has
proven \(*a at time i.  ``\(*a'' is any wff in the \fIagent's\fR
language.  In $SL sub 0$, the ``i'' is just \fIour\fR notation; the agent has no
way of knowing that it is at time i that he has proven \(*a (he just ``knows'' 
\(*a at time i).  We, the scientists, however, can talk about
what the agent has and has not proved through the use of |-, the first-order
turnstile.  For example, it might be the case that we have been able to
prove that the agent has been unable to prove ``P'' in time i, where P
is a predicate letter in the agent's language.  (This in particular would be
the case for an agent using ordinary propositional logic.)
We would
write this |- $\(no sup i roman P$. We will continue the convention of using
Greek letters for agent wffs.  These also serve as terms of AF. To further
distinguish AF and RF, we use ``implies'' and ``not'' as function symbols
of AF to designate implication and negation, respectively, of the agent's wffs.
.pp
We have recently initiated the development of $SL sub 0$. 
$SL sub 0$ does not have S, T, nor R for the agent.
To simplify it even more, it contains only propositions; it has no variables.  
As such, then, it is basically a formalism to help us, as scientists,
to understand the reasoner.  
It does not allow the agent to do any reasoning about his own reasoning.  
We use the notation |- $""sup i \(*a$ to indicate that the agent has proven \(*a in i steps.  
Note that \(*a is a formula in the agent's language, but is treated as a constant 
in our language.  
We can think of ``$"" sup i \(*a$'' as an abbreviation for
Thm(i,\(*a), which refers to statements \(*a that can be proven in the
agent's theory in i steps.
The goal is to design  $SL sub 0$ to be strong enough so
that for each i \(mo \fBN\fR, 
and for each \(*a \(mo L(RF) (where L(RF) is the agent's language), we have
.ce
$SL sub 0$ |- $"" sup i \(*a$   or   $SL sub 0$ |- $\(no sup i \(*a$.  
This is our formal definition of ``analytic completeness''. See [Drapkin&Perlis 1986] for  details
of our current efforts to formalize such a theory $SL sub 0$. As a sample
of the kind of axiom that we have found useful, we mention
the following (similar to one found in [Haas 1985]):
.ce
|-  (\*(qai)(\*(qaj) [ [$"" sup i$ \(*a  & $"" sup j$implies(\(*a,\(*b) & i<k & j<k] \(->  $"" sup k \(*b$]
Intuitively this expresses a version of the rule of \fImodus ponens\fR.
Namely, if the agent ``knows'' (or has established) \(*a at time i, and also
knows at time j that \(*a implies \(*b, then it will know \(*b at any time
k greater than i and j.
.br
\fBIV. Conclusions\fR
.pp
We have argued that for appropriate reasoning in the commonsense world, it
is necessary to keep track of ones own steps of reasoning. Moreover, for
us to be able to study such reasoners, it is necessary to have a formal
description of ``step-reasoning'' so that it will be possible to determine
whether, at a given moment, a given agent has (or has not) come to a certain
conclusion. In the felicitous phrases of Doyle [1982] and Konolige [1985b], we need 
to formalize a ``rational'' or ``robot'' psychology. Here we have suggested
particular avenues for doing just that, together with a ``completeness''
criterion.
.bp
.ls 1
.lp
\fBAcknowledgements\fR
.sp
This research has been supported in part by grants from the following
organizations:
.in +2
.br
The IBM Corporation, for J. Drapkin
.br
U. S. Army Research Office (DAAG29-85-K-0177), for J. Drapkin and D. Perlis
.br
The Martin Marietta Corporation, for D. Perlis
.in -2
.sp
\fBBibliography\fR
.ls 1
.np
Allen, J. [1984] Towards a general theory of action and time. Artificial
Intelligence, \fB23\fR, pp.123-154.
.np
Doyle, J. [1982] Some theories of reasoned assumptions: an essay in rational
psychology. Dept. of Computer Science, CMU.
.np
Drapkin, J., Miller, M., and Perlis, D. [1986] A memory model for real-time
commonsense reasoning. Draft.
.np
Drapkin, J. and Perlis, D. [1986] A preliminary excursion into step logics.
\fIProc. Intl. Symp. on Methodologies for Intell. Systems\fR, Knoxville,
1986.
.np
Fagin, R. and Halpern, J. [1985] Belief, awareness, and limited reasoning:
preliminary report. IJCAI 85, pp.491-501.
.np
Haas, A. [1985] Possible events, actual events, and robots. Computational
Intelligence, pp.59-70.
.np
Konolige, K. [1985a] A computational theory of belief introspection. IJCAI 85,
pp.503-508.
.np
Konolige, K. [1985b] Experimental robot psychology. SRI International,
Technical Note 363.
.np
Lakemeyer, G. [1986] Steps towards a first-order logic of explicit and 
implicit belief. 
\fIProc. 1986 Conference on Theoretical Aspects of Reasoning about 
Knowledge\fR, pp.325-340.
.np
Levesque, H. [1984] A logic of implicit and explicit belief. Proceedings,
3rd National Conf. on Artificial Intelligence, pp.198-202.
.np
McDermott, D. [1982] A temporal logic for reasoning about processes and
plans. Cognitive Science, \fB6\fR, pp.101-155.
.np
Nilsson, N. [1983] Artificial intelligence prepares for 2001. AI Magazine,
\fB4\fR.
.np
Perlis, D. [1981] Language, computation, and reality. Ph.D. Thesis,
University of Rochester.
.np
Perlis, D. [1984] Non-monotonicity and real-time reasoning. Proceedings,
AAAI Workshop on Non-Monotonic Reasoning, New Paltz, Oct. 17-19.
.np
Vardi, M. [1986] On epistemic logic and logical omniscience,
\fIProc. 1986 Conference on Theoretical Aspects of Reasoning about 
Knowledge\fR, pp.293-305.
