\documentclass{article}
\usepackage{times}
\usepackage{eufrak}
\pagestyle{myheadings}


\oddsidemargin=0in
\evensidemargin=0in
\topmargin=0in
\textheight=8.5in
\textwidth=6.5in
%\headheight=0in
%\headsep=0in
\parindent=0in
\parskip=0.1in


\newcommand{\comment}[1]{{\bf #1}}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% implic paper is in  /fs/disco/group/papers/implic/final_implic
% implic code is in /fs/that/kpurang/work/active/implic
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}


We present another application in language processing of the basic
ideas about mistakes. The phenomenon we model here is the cancellation
of implicatures. Implicatures occur all the time in language and dialog,
and sometimes one makes an implicature only to find out later on that
is was a mistake to do so. We tend to smoothly adopt the correct view
and continue with the dialog.

The basic theory accounting for this behavior is Grice's \cite{}
theory of meaning and implicature. Grice's work has been around for a
long time but there has not been any commonly accepted computational
version of his work. The owrk here is an attempt to design algorithms
that will implement a small but rather problematic part of the theory of
implicatures. The problem here involves mistakes: the agent coming to
beleive $\phi$ on hearing an utterance, then rejecting this view ofr
$\neg phi$ on hearing more.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{What is implicature}

%Talk about grice's story a bit- the maxims?

Implicatures, roughly, are propositions that are implied by an
utterance without being entailed by that utterance. If someone says
``John is in the yard or at work.'' an implicature owuld be that the
speaker does not know exactly where John is. This proposition is not
entailed by what the speaker said, but seems to follow from it if we
assume that the speaker is not being evasive.

\subsection{The cooperative principle}

This sort of resonableness assumption is stated in Grice's \cite{}
{\em Cooperative Principle}: 
\begin{quote}
Make your conversational contribution such as is required, at the
stage at which it occurs, by the accepted purpose or direction of the
talk exchange in which you are engaged.
\end{quote}

This is assumed to be the basis upon which people converse. If we did
not conform to such a principle, conversations could degenerate into a
string of non-sequiturs with no useful information exchange.

\subsection{The maxims}

Grice fleshes out the Cooperative Principle into a set of maxims that
fall into 4 categories:
\begin{itemize}
\item[Quantity]
	\begin{itemize}
	\item Make your contribution as informative as is required.
	\item Do not make your contribution more informative than is
	required. 
	\end{itemize}
\item[Quality]
	\begin{itemize}
	\item Try to make your contribution one that is true.
	\item Do not say what you beleive is false
	\item Do not say that for which you lack adequate evidence
	\end{itemize}
\item[Relation]
	\begin{itemize}
	\item Be relevant.
	\end{itemize}
\item[Manner]
	\begin{itemize}
	\item Avoid obscurity of expression.
	\item Avoid ambuguity.
	\item Be brief.
	\item Be orderly.
	\end{itemize}
\end{itemize}

In cooperative conversation, it is reasonable for the participants to
follow these maxims. But they need not always do so, a participant can
\begin{itemize}
\item Violate a maxim, in which case he may mislead the hearer.
\item Opt out of the Cooperative Principle, and make it clear that he
is not going to cooperate in the conversation.
\item Be faced with a clash, when different maxims require conflicting
behavior. 
\item Flout the maxim, in which case the speaker blatantly violates
the maxim.
\end{itemize}

The last item typically gives rise to conversational implicature. The
hearer must reconcile what the person said with the assumption that the
speaker is observing the Cooperative Principle.

\subsection{Conversational implicature}

Grice then says what a conversational implicature is:
\begin{quote}
A man who, by (in, when) saying (or making as if to say) that $p$ has
implicated that $q$, may be said to have conversationally inokicated
that $q$ provided that (1) he is presumed to be observing the
conversational maxims, or at least the cooperation principle; (2) the
supposition that he is aware that, or thinks that, $q$ is required in
order to make his saying or making as if to say $p$ (or doing so in
those terms) consistent with the presumption; (3) the speaker thinks
(and would expect the hearer to think that the speaker thinks) that it
is within the competence of the hearer to work out, or grasp
intuitively, that the supposition mentioned in (2) is required.
\end{quote}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The problem}

Our main concern here is to model the making and withdrawal of
implicatures in real time as a conversation proceeds. As mentioned
above, implicatures are not entailed by the utterance, and so it is
possible that one makes an implicature based on the assumption the the
speaker is following the Cooperative Principle and later find that
that implicature has to be withdrawn. For instance, if one asks
whether it rained last night, the response might be ``The grass is
wet $\ldots$'' from which one gets the implicature that it did rain,
but if the speaker continues with ``$\ldots$ but the street is not
wet.'', it seems false that it did rain and the implicature has to be
withdrawn. This is another instance of a mistake: the initial beleif
that it did rain was mistaken and we detect that mistake once we learn
that the street is not wet. If the grass is wet and the street not,
then it is likely that it did not rain and the sprinkler was on.

As conversation proceeds, we make many implicatures and some of them
turn out to be mistaken. The KB of an agent will vary over time with
the addition and removal of implicatures. It is this change in the
beleifs of the agnet over time that we want to model.

This work is similar to the presuppositions work discussed
earlier. The utterances in the conversation invite us to make some
assertions that turn out to be false. In this case though, there is no
need to maintain an explicit context for the conversation.


We illustrate the problem and our solution to it with two simple examples.

\subsection{Example 1}

Consider the following conversation fragment:

\noindent
(A)     Kathy: {\bf Are the roses fresh?}\\
\noindent
(B)     Bill: {\bf They are in the fridge.}\\
\noindent
(C)     Bill: {\bf But they're not fresh.}\\

We aim to model Kathy's reasoning in response to Bill's
utterances. After utterance (B), Kathy concludes that the roses are
fresh. If Bill is being cooperative and obeying to the maxim of
relevance, then the fact that the roses are in the fridge is relevant
to Kathy's question about their freshness. The possible link is that
fridges are typically used to keep things fresh and so Kathy can
conclude that Bill wants her to conclude that the roses are indeed
fresh. 

However, at (C), Bill says that they are not fresh. This is not
consistent with the implicature that Kathy made and since the
implicature is not entailed by (B), Kathy prefers (C) and concludes
that the roses are not fresh.

Kathy's beleif about the roses' freshness starts out being unknown,
true, then false. The problem is to model this wihthout bad things
happenning, like cocnluding all formulas in the language.


\subsection{Example 2}

\noindent
(A)     Kathy: {\bf Are the roses fresh?}\\
\noindent
(B)     Bill: {\bf They are in the fridge.}\\
\noindent
(D)     Bill: {\bf But they are old.}\\

As in the first example, (B) implicates that the roses are fresh. But
then (D) seems to implicate that they are not fresh. Kathy may then
decide to go with either of the implications or none of
them. This seems to depend on Kathy's beleif about the power of
fridges to keep roses fresh. If she has no opinion on that, then she
will have no opinion onthe freshenss of the roses either after this
dialog. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our solution}

Our solution to this problem is similar to the solution for the
presupposition case as far as handling mistakes is concerned. The
problem-specific processing and the representation of the information
is different.

\subsection{Representation}

THe representation in this case is quite different from the
representation in the presupposition case. 

\subsubsection{Operators}

There are a few new operators:
\begin{itemize}
\item \verb+ => + is used to specify implications. 
\item \verb+ & + is used for conjucntion.
\item \verb+ @ + is used to state the time at which a formula is
taken to be true. $\phi@10$, for instance, expresses that $\phi$ is
taken to be true at step 10.
\item \verb+ ? + This is a unary predicate which is true if its argument is in
the KB at the time that it is computed.
\item \verb+ ` + THis is similar to ``?'' except that it searches for an
exact match of its aregument in the KB. Unification is not allowed.
\item \verb+ : + This is used to associate formulas to ``contexts'' or
groupings of formulas. Formulas are grouped together according to
theor purpose. $bel:\phi$, for instance says that $\phi$ is a beleif
whereas $imp:\phi$ represents that $\phi$ is an implicature. The
grouping used are:
\begin{itemize}
\item [bel] for beliefs.
\item [time] for time facts
\item [utt] for utternaces
\item [rel(P)] for formulas that are relevant to $P$
\item [mt] for control facts
\end{itemize}
\end{itemize}

\subsubsection{Some formulas}

We present some formulas and their intended meaning to clarify the
syntax: 
\begin{itemize}
\item \verb+ time:now(1) + says that the current step is 1
\item \verb+ bel:roses(r1) + says that the agent believes that there
are roses called $r1$
\item \verb+ utt: (q_yes_no(kathy, bill,fresh(r1))@1) + says that the
agent beleives that there was an utterance at time 1 which was a
yes/no question from Kathy to Bill about whether $r1$ was fresh.
\end{itemize}



\subsection{Axioms used.}

The first set of axioms for inheritance, contradiction detection and
resolution, and the clock rule are likely to be common to a wide
variety of problems described using this representation.

\begin{verbatim}
% inheritance rule: we inherit anything that is not killed and is not
%     itself a kill
(mt:(((`(Q:X)) & (?(kill(X)))  & eval(\+ (Q = time)) & (?(k2(Q))) 
      & eval(\+ (X = k2(_))) & eval(\+ (X = kill(_)))) => (Q:X)))@0.

%contradiction detection rule: when we detect a contradiction, we add
%     a contra belief at the next step and we kill both contradictands
%     and the contra belief itself (so these don't propagate)

(mt:((`(Q:X) & `(W:not(X)) & (?(kill(not(X))))) => 
    mt:(kill(not(X)))))@0.
(mt:((`(Q:X) & `(W:not(X)) & (?(kill(contra((Q:X), (W:not(X))))))) => 
    mt:(kill(contra((Q:X), (W:not(X)))))))@0.
(mt:((`(Q:X) & `(W:not(X)) & (?(kill(contra((Q:X), (W:not(X))))))) => 
    mt:(contra((Q:X), (W:not(X))))))@0.
(mt:((`(Q:X) & `(W:not(X)) & (?(kill(X)))) => mt:(kill(X))))@0.

%contradiction resolution rule: we prefer utterances(beliefs)

(mt:((contra((imp:X), (bel:Y))) => (bel:Y)))@0.
(mt:((contra((bel:Y), (imp:X))) => (bel:Y)))@0.

% the clock rule
(mt: ((`(time:now(T)) & eval(T1 is T+1)) => time:(now(T1))))@0.

% the initial time
(time: (now(0)))@0.
\end{verbatim}

The next set involves specific rules for the implicature problems:

\begin{verbatim}

% usually when X informs us of P, we believe P.
(bel: (( (inform(X, Y, P)@_) & whoami(Y) & (?(ab2(X, Y, P)))
         ) => (bel:P)))@0.

% direct response to a yes-no question- we believe the response
(bel: (( (respond(X, Y, P, q_yes_no(P))@_) & whoami(Y) & (?(ab2(X, Y, P)))
           ) => (bel:P)))@0.
(bel: (( (respond(X, Y, not(P), q_yes_no(P))@_) & whoami(Y) & 
         (?(ab2(X, Y, P)))  ) => (bel:P)))@0.

% indirect responses to yes-no questions- we try to figure what it means
(bel: (( (respond(_, X, P, q_yes_no(Q))@T) & whoami(X) & now(T) &
          eval(\+ P = Q) & eval(\+ P = not(Q)))
      => (qyn(Q):P) ))@0.

% if we have figured the answer, we make it an implicature and we stop
% trying to find out what the response meant. In this case, we just lose
% all the irrelevant beliefs we came across.
(bel: (( `((qyn(Q)):P) & (?(k2(qyn(Q)))) & eval(P = Q)) => imp:P))@0.
(bel: (( `((qyn(Q)):P) & (?(k2(qyn(Q)))) & eval(P = Q)) => 
    mt:k2(qyn(Q))  ))@0.
(bel: (( `((qyn(Q)):P) & (?(k2(qyn(Q)))) & eval(P = not(Q))) => imp:P))@0.
(bel: (( `((qyn(Q)):P) & (?(k2(qyn(Q)))) & eval(P = not(Q))) => 
    mt:k2(qyn(Q))  ))@0.

\end{verbatim}

Finally we have rules that are specific for the examples under
consideration: 

\begin{verbatim}

% things in fridges are cold
(bel: ((`(Z:infridge(X)) & (?(k2(Z))) & (?(not(cold(X))))) => Z:cold(X)))@0.

% things in fridges are small
(bel: ((`(Z:infridge(X)) & (?(k2(Z))) & (?(not(small(X))))) => Z:small(X)))@0.

% things in fridges are dead
(bel: ((`(Z:infridge(X)) & (?(k2(Z))) & (?(not(dead(X))))) => Z:dead(X)))@0.

% things in fridges are edible
(bel: ((`(Z:infridge(X)) & (?(k2(Z))) & (?(not(edible(X))))) => 
       Z:edible(X)))@0.

% cold roses are fresh
(bel: ((roses(X) & `(Z:cold(X)) & (?(k2(Z)))  & (?(not(fresh(X)))))
       => Z:fresh(X)))@0.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% initial beliefs

(bel: (whoami(kathy)))@0.
(bel: (roses(r1)))@0.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{verbatim}

The utterances considered are also represented in the same way. These
are presented later.




\subsection{Example 1}

For example 1, we have these additional formulas:

\begin{verbatim}
% utterances

% kathy asks bill whether the roses are fresh or not at time 1
(utt: (q_yes_no(kathy, bill, fresh(r1))@1))@1.

% bill responds to kathy's question by saying that the fridges are in the
% fridge (at time 2)
(utt: (respond(bill, kathy, infridge(r1), q_yes_no(fresh(r1)))@2))@2.

% then, at time 7, bill tells kathy that the roses are actually not fresh.
(utt: (inform(bill, kathy, not(fresh(r1)))@7))@7.

\end{verbatim}

Some of the output trace for this case is presented below.



Our implementation in active logic produces the positing of an implicature
followed by its cancellation as we move from (B) to (C). There are several steps
between (B) and (C) and several more after (C).  The output trace below shows
that at step 9 (which occurs after the utterance (C)) a contradiction briefly
appears. Then it is withdrawn. At this point Kathy has no belief at all whether
or not the roses are fresh. Then the belief that they are not fresh is restored
using a rule that expresses the maxim of Quality. Below we reproduce and comment
in more detail on some of the output trace for this example.

\subsubsection{Step 1}

\begin{verbatim}
time:now(1)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
\end{verbatim}

Kathy asks (A) {\bf Are the roses fresh?}

\subsubsection{Step 2}
\begin{verbatim}
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
bel:whoami(kathy)
bel:roses(r1)
time:now(2)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2
\end{verbatim}

Bill says (B) {\bf They are in the fridge}.


\subsubsection{Step 3}
\begin{verbatim}
rel(fresh(r1)):infridge(r1)
bel:infridge(r1)
time:now(3)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
\end{verbatim}

Kathy starts to think about what Bill's response means. The proposition {\bf
rel(fresh(r1)):infridge(r1)} means some thing like: With regard to the question
whether or not {\bf fresh(r1)} the fact that {\bf infridge(r1)} could be
relevant. The active logic system, in the course of its normal inheritance
procedure will try to fire any rules that it can using {\bf infridge(r1)}. This
is our simplified model for Kathy's thinking about or trying to figure out the
relevance of Bill's indirect answer.


\subsubsection{Step 4}

\begin{verbatim}
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
bel:whoami(kathy)
bel:roses(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
rel(fresh(r1)):infridge(r1)
time:now(4)
bel:infridge(r1)
rel(fresh(r1)):cold(r1)
rel(fresh(r1)):small(r1)
rel(fresh(r1)):dead(r1)
rel(fresh(r1)):edible(r1)
\end{verbatim}

Nothing interesting yet. Kathy is still thinking. She considers the
relevance of the temperature, size, and edibility of the roses.

\subsubsection{Step 5}
\begin{verbatim}
rel(fresh(r1)):fresh(r1)
time:now(5)
rel(fresh(r1)):edible(r1)
rel(fresh(r1)):dead(r1)
rel(fresh(r1)):small(r1)
rel(fresh(r1)):cold(r1)
bel:infridge(r1)
rel(fresh(r1)):infridge(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
\end{verbatim}

Here at step 5 Kathy finds that the roses are fresh. That is the import of

\begin{verbatim} 
rel(fresh(r1)):fresh(r1).
\end{verbatim}
 

Inferring either this or 

\begin{verbatim}
rel(fresh(r1)):not(fresh(r1))} 
\end{verbatim}

\noindent
will stop Kathy's search for an answer to her yes--no question.


\subsubsection{Step 7}
\begin{verbatim}
time:now(7)
imp:fresh(r1)
bel:infridge(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
utt: (respond(bill,kathy,not(fresh(r1)), q_yes_no(fresh(r1)))@7)
\end{verbatim}




Now Bill says (C) {\bf The roses are not fresh}.

\subsubsection{Step 9}
\begin{verbatim}
time:now(9)
mt:kill(fresh(r1))
mt:contra(imp:fresh(r1), bel:not(fresh(r1)))
mt:kill(contra(imp:fresh(r1), bel:not(fresh(r1))))
mt:kill(not(fresh(r1)))
bel:infridge(r1)
bel:not(fresh(r1))
imp:fresh(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
utt: (respond(bill,kathy,not(fresh(r1)), q_yes_no(fresh(r1)))@7)
\end{verbatim}

Kathy believes what Bill says, {\bf bel:not(fresh(r1))} but now
detects a contradiction. So neither the implicature {\bf
imp:fresh(r1)} nor this belief will inherit to the next step.

\subsubsection{Step 10}
\begin{verbatim}
utt: (respond(bill,kathy,not(fresh(r1)), q_yes_no(fresh(r1)))@7)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
bel:whoami(kathy)
bel:roses(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
time:now(10)
bel:not(fresh(r1))
bel:infridge(r1)
\end{verbatim}

But the belief that the roses are not fresh is derivable from {\bf
utt:(inform(bill,kathy, not(fresh(r1)))@7)} at step 9 because Kathy
believes everything Bill said. However, the implicature from (B), that
the roses are fresh cannot be rederived; an utterance initiates a
search for relevance and hence implicature only at the step it is
perceived. In active logic we spread the reasoning over a sufficient
number of steps to, so to speak, divide and conquer some of the
complex thinking that happens during discourse understanding.


\subsection{Example 2}


The common background includes the defeasible belief that old roses
are not fresh. We saw in the first example that the implicature at (B)
was based on defeasible beliefs about things in fridges and about cold
roses.  This means that we could have a Nixon Diamond after (D); there
will be two (defeasible) implicatures: one that the roses are fresh
and the other that they are not fresh. This is what happens in our
trace as we pick up Kathy's thinking at step 7.

\subsubsection{Step 7}

\begin{verbatim}
time:now(7)
imp:fresh(r1)
bel:infridge(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
utt: (respond(bill,kathy,old(r1), q_yes_no(fresh(r1)))@7)
\end{verbatim}

Here is where Bill says (D) {\bf They are old}.

\subsubsection{Step 8}

\begin{verbatim}
utt: (respond(bill,kathy,old(r1), q_yes_no(fresh(r1)))@7)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
bel:whoami(kathy)
bel:roses(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
imp:fresh(r1)
time:now(8)
bel:old(r1)
bel:infridge(r1)
rel(fresh(r1)):old(r1)
\end{verbatim}

As before, Kathy begins to think about the relevance to her question of Bill's
utterance. So she now thinks that the age of the roses is relevant to her
question {\bf rel(fresh(r1)):old(r1)}. She will begin searching for what that
relevance could mean.

\subsubsection{Step 9}

\begin{verbatim}
rel(fresh(r1)):not(fresh(r1))
time:now(9)

rel(fresh(r1)):old(r1)
bel:infridge(r1)
bel:old(r1)
imp:fresh(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
bel:roses(r1)
bel:whoami(kathy)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
utt: (respond(bill,kathy,old(r1), q_yes_no(fresh(r1)))@7)
\end{verbatim}

She happens to discover the relevance after only one step. It is that the roses
are not fresh {\bf rel(fresh(r1)):not(fresh(r1))}.

\subsubsection{Step 10}

\begin{verbatim}
utt: (respond(bill,kathy,old(r1), q_yes_no(fresh(r1)))@7)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
bel:whoami(kathy)
bel:roses(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
imp:fresh(r1)
rel(fresh(r1)):old(r1)
mt:kill(not(fresh(r1)))
mt:kill(contra(imp:fresh(r1), rel(fresh(r1)):not(fresh(r1))))
mt:contra(imp:fresh(r1), rel(fresh(r1)):not(fresh(r1)))
mt:kill(fresh(r1))
time:now(10)
bel:old(r1)
bel:infridge(r1)
mt:k2(rel(fresh(r1)))
rel(fresh(r1)):not(fresh(r1))
\end{verbatim}          

Here the contradiction is discovered between the the implicature from
(B) and the more recent implicature from (D). So both are marked {\bf
kill} so they will not be inherited at the next step.
                                
\subsubsection{Step 12}

\begin{verbatim}
utt: (respond(bill,kathy,old(r1), q_yes_no(fresh(r1)))@7)
utt: (q_yes_no(kathy,bill,fresh(r1))@1)
bel:whoami(kathy)
bel:roses(r1)
utt: (respond(bill,kathy,infridge(r1), q_yes_no(fresh(r1)))@2)
time:now(12)
bel:old(r1)
bel:infridge(r1)
\end{verbatim}

Here Kathy has reached a state where she cannot infer anything helpful
about her original question. It worked out this way because Bill said
things which only implicated that the roses were fresh or not
fresh. But both implicatures were cancelled. Active logic {\sl can}
allow one to infer again what one has just withdrawn. But in the model
we have designed the search for the relevance or import of an
utterance only begins at the step immediately following the perception
of the utterance as relevant to a question. This captures the fact
that it is utterances, not beliefs, that give rise to implicatures. It
also captures the idea that cancellation just eliminates the
implicature; any new implicature would have to wait for a new
utterance. However, we are not prepared to say that this strategy is
generally applicable.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion of the solution}

This problem, the solution and the representation of the solution
differ from those in the presupposition problem case. However, the
part of the solution relating to mistakes is similar.

In this example as well as the previous one, the detection of a
mistake occurs when a direct contradiction is found. This says that
there is a mistake somewhere, but it is not clear what the mistake
is. This logic, just like that one for the presuppositions, prevents
the potentially mistaken formulas from being used to derive new
formulas. The way it does it in this case though, is to disinherit it
so that it does not appear in future steps.

The specific strategy to decide which beleif is mistaken depends on
the domain. In this implicature domain, the strategy is simply to
prefer beleifs over implicatures. This is expressed in:

\begin{verbatim}
(mt:((contra((imp:X), (bel:Y))) => (bel:Y)))@0.
(mt:((contra((bel:Y), (imp:X))) => (bel:Y)))@0.
\end{verbatim}

If there is a contradiction between an implicature $X$ and a belief
$Y$, then $Y$ will be added at the next step. Since once a
contradiction is noticed the formulas are disinherited, the
implicature is not present in the next step.

At this point, we have discovered what the mistake is: it is the
implicature. This is not explicitly asserted in the database in this
simple approach to mistakes. It is sufficient, in this example, for the
mistaken beleifs not to be present in the subsequent steps and that is
accomplished automatically.

In the second example, we have a contradiction between two
implicatures. In our simple approach, there is no way to resolve such
a contradiction so both contradictands remain dissinherited. In this
case we have not identified what the mistake is: whether it is that
the roses are fresh or that they are not fresh.

Once again, if there had been actions taken on the basis of the
mistaken beliefs or if we had recursive mistakes, it would have been
necessary to explicitly assert and reason about the mistaken
beliefs. The example, as the previous one, shows, however, that in
cases where the behavior is limited, one can get by with a much
simpler way to handle mistakes.



\end{document}



