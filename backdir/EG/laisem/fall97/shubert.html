<HTML>
<HEAD>
<TITLE>
Shubert talk
</TITLE>
<BODY BGCOLOR = "eeeeee" BACKGROUND="bg.gif">
</HEAD>

<p>
November 21 1:30 pm
<p>
A Computational Model of Belief
<p>
Len Schubert, University of Rochester
<p>

Anticipating people's beliefs (and desires, etc.) is crucial for
intelligently interacting with them, as well as for understanding
narratives. Traditional methods for reasoning about beliefs, based on
possible-worlds semantics, assume that people believe all of the
consequences of their beliefs. But in reality, ``inferential belief
retrieval" appears to be a sharply time-limited, complex process,
skewed toward certain kinds of inference. For example, most people
would say they know that Copenhagen is in the Northern hemisphere,
though this is unlikely to be a directly ``memorized" fact; but most
would not be able to say whether 97 is a prime until they have given
it some thought, even though only a few reasoning steps are required.
This observation suggests that it would be utterly impractical to base
reasoning about beliefs on classical deduction: we would have to have
an *axiomatic* model of the intricate mental machinery used in
inferential belief retrieval. Even if we could formulate such a model,
the difficulty of drawing conclusions about beliefs would render it
virtually useless. But there is another strategy available to a
thinker with the *same* mental machinery as the agent whose beliefs
are of interest: he can simply *run* the machinery on (what he takes
to be) the other agent's beliefs, and observe the results. In
principle this should give a ``real-time" method of belief
inference. The question arises whether such a computational model can
be used as a formal basis for the semantics of belief, and under what
conditions simulating inferential belief retrieval of another agent is
sound. I will describe such a formal model, and conditions for
soundness of simulative inference. Certain incompleteness and
restricted completeness results can be proved as well. Many
interesting questions, e.g., about simulating uncertain or default
inference, or predicting belief change (e.g., as a result of receiving
new information) remain open.
<p> 
This is joint work with Aaron Kaplan.



</html>

