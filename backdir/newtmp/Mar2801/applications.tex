%\documentclass{article}

%\begin{document}

As indicated above, active logic provides a framework for reasoning in
presence of uncertainties.  Some of the
application areas of active logic are discussed below.

\subsection{Deadline-Coupled Planning}
Dead-line coupled planning involves taking into
account the uncertainties that could crop up in the planning process,
while at the same time factoring in the ever decreasing time to deadline.
%While generating plans that involve deadlines, the approaching
%	deadline has to be factored into the planning process as an
%	evolving aspect  so
%	that the generated plan will remain feasible for application. 
        Consider for example, an agent planning a strategy to FedEx a hard-copy of
	a paper before the deadline. While the agent is
	planning the strategy to get to the nearest FedEx location, the clock is ticking. 
	Therefore the time he has available to reach the location before it closes is fast 
	decreasing. While he is trying to reach the nearest
	FedEx location, time is still passing and hence many uncertain
	events could happen which could mandate more 
	planning. For instance, a traffic jam could delay the agent and
	the nearest FedEx location might get closed by then so that he
	will have to go to another location which is open for 24
	hours. 

 The time tracking and observation mechanisms of active logic render it useful in such
applications that deal with uncertainties while trying to meet a deadline.{Nirkhe et al. 1997}


\subsection{Common Sense Reasoning}
Active logic finds applications from fully-decidable default reasoning
to reasoning in the presence of contradictions. Some examples are
listed below.

\subsubsection{Simple Default Reasoning}
	Given facts {\em Birds generally fly} and {\em Tweety is a bird},
	active logic can conclude {\em Tweety flies}.
	
\subsubsection{Default Reasoning with Preferences}
	In active logic one can specify default preferences like  ``{\em
	Penguins do not fly} is preferred over {\em Birds generally
	fly}''. Then, if at any instant, {\em Birds generally fly.}
	{\em Tweety is a bird.} {\em Tweety is a
	penguin.} and {\em Penguins do not fly.} are in the database,
	active logic can conclude {\em Tweety does not fly}.

\subsubsection{Maintaining world view}
An accurate world view cannot be specified with apriori knowledge
alone, because of the uncertainty aspect in current knowledge. Current knowledge
can have gaps (eg., not knowing what constitutes black holes) or it may even be wrong (eg., earth is flat).  As time evolves, 
facts might change or cease to be true (for eg., the current
president, cold war) or even new facts might arise (for eg., existence of
International Space Station).  In order to deal with the ever changing
plethora of facts, active logic has mechanisms to add, modify or
delete facts on the fly.

\subsubsection{Reasoning with Contradictions}
Traditional logics generate all consequences in the presence of
contradictions, whereas active logic uses contradictions to help in
its reasoning 
process. An agent can believe that Tweety flies until he gets contrary
information through observation or reasoning. In the presence of
contradiction, active logic distrusts both the contradictands (Tweety
flies and Tweety does not fly) until it has enough facts to
trust one over the other.

%\subsubsection{Reasoning with Mistake Recognition}
%In some applications it is mandatory for the system to realize that a
%		     mistake was committed. Consider the situation where 
%the general rule is that if Alex
%		     is at school at 3:30 he has to be picked
%		     up. Suppose, at 3:30, Mary believed that Alex was not at
%		school and hence didnot initiate the action required
%		to pick him up. In order that Alex gets picked up at a
%		     later time, it is mandatory that Mary realizes
%		     that she made a mistake at 3:30 in believing that
%		     Alex was not at school. 
%The contradiction handling and time tracking mechanisms of active logic provide
%		     facilities to recognize a mistake, identify the
%		     time at which the mistake occured and initiate
%		     corrective action if required.





\subsection{Dialog}
	It is often the case that uncertainty arises when Cooperative
	Principle (Grice 1975) is not observed among the discourse
	participants.  For instance, when the speaker provides
	insufficient amount of information, or when it is false,
	irrelevant, ambiguous, vague, or when it lacks adequate
	evidence, the addressee is uncertain about the speaker's
	intention.  In some cases, conversation or communication just
	stops there, maybe because the speaker is infelicitous and the
	addressee does not wish to participate in conversation
	anymore.  In most cases, however, the addressee reasons about
	the speaker's intention and tries to stay in conversation.
	Despite the fact that there is a potential risk of
	misunderstanding that could lead to further uncertainty, we take advantage of reasoning 
	when it comes to uncertainty resolution.
	Hence, in computation, using intelligent reasoning seems to be one method to deal with uncertainty.

%is a matter	of course.  

In the following subsections, we will discuss how
	intelligent reasoning can be effectively woven into
	uncertainty resolution in the context of dialog applications.


\subsubsection{Meta-Dialogue}
When uncertainty arises, one way to avoid further uncertainty and
potential discourse failure is to clarify it.  In natural language
discourse, clarification takes place at all times.  For instance, when
someone says {\em I went to Springfield yesterday} and you realize
(via introspection) that you are unsure which State is meant, you may
ask {\em In which state?}. 
We have spliced this kind of reasoning into the Rochester TRAINS \cite{trains...}
system, so that when it is unable to understand which train that 
the user refers to, it returns a message that says, {\em Please
specify the name of the train.} Furthermore, if the user rejects the
system's choice of train by saying ``no'' and then repeats the exact
same request (eg, ``send the Chicago train to New York''), the
(active-logic-modified system) will recognize a contradiction between
the rejection (don't do X) and the repeat (do X) and will then look
for a repair (eg, it has picked out the wrong referent of ``Chicago
train''). 


\subsubsection{Default Meanings}
The relationship between an expression and its meaning is
conventional and arbitrary.  Hence, discourse participants need to be able to understand at least nonnatural meanings (Grice 1957) that forms/expressions refer to as default meanings.  For example, ringo (Japanese)
means (nonnatural) {\em fruit of a plant of the sub-genus malus.}  If you are not aware of this Japanese term, you are not able to retrieve the meaning.  Those who are aware of the term, however, will retrieve the nonnatural meaning as a default meaning.  Another example is that in the U.S. the default (nonnatural) meaning of green (in the street light) is {\em go.}  In Japan, however, it is blue (in the street light) that has the same meaning.  Discourse participants reason about their discourse environment, in which default meanings are assessed.


\subsubsection{Assumption}
For uncertainty resolution, people make assumptions as natural
consequence.  Grice (1957) explains natural consequence in terms of
natural meaning.  For instance, {\em Sandy is allergic to pollen}
means (natural) she will not want her flowers in her office (Green
1989:7).  It is not the case, however, that natural meaning is always felicitous.  Despite the fact that she is allergic to pollen, she may want her flower in her office, in which case, the assumption based on natural meaning is incorrect.  It is important that the system is able to repair false assumptions (see 'Repairs' below).

In ambiguous cases, people make assumptions based on previous knowledge they
have.  The expression {\em the Chicago train} can be ambiguous because
any train that is associated with Chicago can be a potential referent
of the expression: it may be a train that departed Chicago, or it may
be a train whose destination is Chicago.  In the TRAINs system, the
system can make an assumption that the most-recently referred train
that is associated with Chicago can be referred to as {\em the Chicago train.}  If the user denies the system's assumption, the system considers alternative candidates.


\subsubsection{Implicature}
Each expression we utter can mean more than it literally means.  For
instance, {\em It is cold here} can mean such as  {\em Close the
window,} {\em Let's leave,} {\em Turn on the heater.}  One way to handle implicature is to assign a default interpretation to the expression.  The better way, however, is for the system to be able to reason about the context, and provide an interpretation that is most fitting for the context.


\subsubsection{Repair}
When false assumptions are made, it is important for the intelligent
system to be able to repair the mistake as we do in natural language
discourse.  In the implicature example above, the addressee B thinks
that the speaker A does not want to stay in the room, for example.
Then B may suggest them to leave the room, and B starts walking out
from the room.  If A's intention is for B to close the
window (if it is a cold, winter night and if the window is open) and
turn the heater on (if there is one), A has to stop B from leaving the
room, and clarify his intention of staying in the room.

Suppose there are two trains at Chicago (Northstar and Metroliner) in the TRAINs system, and the user utters the expression "the Chicago train" to mean Northstar.  Yet, if the system understands the same expression to refer to Metroliner because it happens to be the most-recently referred train that is associated with Chicago (see Assumptions above), the user can deny the system's infelicitous assumption.  Active logic acknowledges the mistake, and alternative candidates are considered; in this case, Northstar to be the referent of "the Chicago train."

When the discourse participants realize misunderstanding (that is not
necessarily due to false assumptions), repair will be made as well.
For example, B initially understands A's use of {\em Bush} to mean
{George Walker Bush}, the 43rd president of the United States,
although A is talking about his father {\em George Herbert Walker
Bush}, the 41st president.  A then tells B that {\em Bush} is the
"read my lips" president.  Suppose B does not know anything about the "read
my lips" speech that the 41st president made, so B understand the 43rd
president to be the "read my lips" president.  Later on when A tells B
that {\em Bush} is married to Barbara, B realizes his
misunderstanding.  At this point, B has to re-associate previous
information with the appropriate referent; B then understands that the
"read my lips" president is the 41st president.   

Another example is contradiction to the previous belief.  For example, if Tweety is a bird, active logic may believe that Tweety can fly based on an assumption that birds generally fly.  If it finds out later that Tweety is a penguin, contradiction arises because penguins do not fly.  In this case, active logic acknowledges that the present information (Penguins do not fly) is preferred over the previous belief (Birds generally fly) and makes an intelligent decision that Tweety does not fly.

%\end{document}
