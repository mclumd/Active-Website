\documentclass{article}

\begin{document}

\section{Applications}

Active logic provides a framework for intelligent reasoning in
presence of uncertainties. Since
intelligent behavior can be exhibited using intelligent reasoning,
Active logic is useful in application areas,
where agents need to exhibit intelligent behavior even in the presence
of uncertainties. Some of the
applications of Active logic are discussed below.

\subsection{Deadline-Coupled Planning}
While generating plans that involve deadlines, the approaching
	deadline has to be factored into the planning process as an
	evolving aspect  so
	that the generated plan will remain feasible for application. Consider for
	example, an agent planning a strategy to submit a hard-copy of
	a paper before the deadline. There are several options before the
	agent, (i) take a flight to deliver it personally, (ii)send it by
	snail-mail, (iii) Fed-Ex the submission. While the agent is
	planning the strategy and deciding which option to choose,
	the clock is ticking. During this time, the last flight might
	have already taken
	off, the post office might have closed and probably only FedEx
	is the only option remaining now. While he is trying to reach the nearest
	FedEx location, time is still passing and hence many uncertain
	events could happen which could mandate creating more
	plans. For instance, a traffic jam could delay the agent and
	the nearest FedEx location might get closed by then so that he
	will have to go to another location which is open for 24
	hours. 


Dead-line coupled planning involves taking into
account the uncertainties that could crop up in the planning process, while at the same time factoring in the ever decreasing time
to deadline. The time tracking and observation mechanisms of Active logic render it useful in such
applications that deal with uncertainties while trying to meet a
deadline.{Nirkhe et al. 1997}

\subsection{Common Sense Reasoning}
Active logic finds applications from fully-decidable default reasoning
to reasoning in the presence of contradictions. Some examples are
listed below.

\subsubsection{Simple Default Reasoning}
	 Given facts {\em Birds generally fly} and {\em Tweety is a bird},
	Active logic concludes {\em Tweety flies} using Modus Ponens.
	
\subsubsection{Default Reasoning with Preferences}

	In Active logic one can specify default preferences like  ``{\em
	Penguins do not fly} is preferred over {\em Birds generally
	fly}''. Then, if at any instant, {\em Birds generally fly.}
	{\em Tweety is a bird.} {\em Tweety is a
	penguin.} and {\em Penguins do not fly.} are in the database,
	Active logic can conclude {\em Tweety does not fly}.

\subsubsection{Reasoning with new inputs/observations}
In most applications, reasoning cannot proceed with apriori knowledge
alone, since most of the knowledge available at any instant is
essentially uncertain. The uncertainty arises because of (i) knowledge
gaps (eg., black holes)  (ii) change in world or (iii) wrong
knowledge (eg., earth is flat).  As time evolves
facts might change or cease to be true (for eg., the current
president, cold war) or even new facts might arise (for eg., existence of
International Space Station).  In order to deal with the ever changing
plethora of facts, Active logic has mechanisms to add, modify or
delete new facts.

\subsubsection{Reasoning with Contradictions}
Traditional logics generate all consequences in the presence of contradictions, where
as Active logics use contradictions to help in its reasoning
process. An agent can believe that Tweety flies until he gets contrary
information through observation or reasoning. In the presence of
contradiction, Active logic distrusts both the contradictands (Tweety
flies and Tweety does not fly) until it has enough facts to
trust one over the other.

\subsubsection{Reasoning with Mistake Recognition}
In some applications it is mandatory for the system to realize that a
		     mistake was committed. Consider the situation where the general rule is that if Alex
		     is at school at 3:30 he has to be picked
		     up. Suppose, at 3:30, Mary believed that Alex was not at
		school and hence didnot initiate the action required
		to pick him up. In order that Alex gets picked up at a
		     later time, it is mandatory that Mary realizes
		     that she made a mistake at 3:30 in believing that
		     Alex was not at school. 
The contradiction handling and time tracking mechanisms of Active logic provide
		     facilities to recognize a mistake, identify the
		     time at which the mistake occured and initiate
		     corrective action if required.





\subsection{Dialog}
	It is often the case that uncertainty arises when Cooperative
	Principle (Grice 1975) is not observed among the discourse
	participants.  For instance, when the speaker provides
	insufficient amount of information, or when it is false,
	irrelevant, ambiguous, vague, or when it lacks adequate
	evidence, the addressee is uncertain about the speaker's
	intention.  In some cases, conversation or communication just
	stops there, maybe because the speaker is infelicitous and the
	addressee does not wish to participate in conversation
	anymore.  In most cases, however, the addressee reasons about
	the speaker's intention and tries to stay in conversation.
	Despite the fact that there is a potential risk of
	misunderstanding that leads to further uncertainty, we take advantage of reasoning when it comes to uncertainty resolution.
	Hence, in computation using intelligent reasoning is a matter
	of course.  In the following subsections, we will discuss how
	intelligent reasoning can be effectively woven into
	uncertainty resolution.


\subsubsection{Meta-Dialogue}
When uncertainty arises, one way to avoid further uncertainty and
potential discourse failure is to clarify it.  In natural language
discourse, clarification takes place at all times.  For instance, when
someone says {\em I went to Springfield yesterday} and you fail to reason about the city of which state, you may ask {\em In which state?}.
In our TRAINS system, when it is unable to understand the train that
the user intends to refer to, it returns the message that says, {\em Please specify the name of the train.}


\subsubsection{Default}
The relationship between an expression and its meaning is
conventional and arbitrary.  Hence, discourse participants need to be able to understand at least nonnatural meanings (Grice 1957) that forms/expressions refer to as default meanings.  For example, ringo (Japanese)
means(nonnatural) {\em fruit of a plant of the sub-genus malus.}  If you are not aware of this Japanese term, you are not able to retrieve the meaning.  Those who are aware of the therm, however, will retrieve the nonnatural meaning as a default meaning.  Another example is that in the U.S. the default (nonnatural) meaning of green (in the street light) is {\em go.}  In Japan, however, it is blue (in the strret light) that has the meaning.  Discourse participants reason about their discourse environment, in which default meanings are assessed.


\subsubsection{Assumption}
For uncertainty resolution, people make assumptions as natural
consequence.  Grice (1957) explains natural consequence in terms of
natural meaning.  For instance, {\em Sandy is allergic to pollen}
means(natural) she will not want her flowers in her office (Green
1989:7).  It is not the case, however, that natural meaning is always felicitous.  Despite the fact that she is allergic to pollen, she may want her flower in her office, in which case, the assumption based on natural meaning is incorrect.  It is important that the system is able to repair false assumptions (see 'Repairs' below).

In ambiguous cases, people make assumptions based on previous knowledge they
have.  The expression {\em the Chicago train} can be ambiguous because
any train that is associated with Chicago can be a potential referent
of the expression: it may be a train that departed Chicago, or it may
be a train whose destination is Chicago.  In the TRAINs system, the
system can make an assumption that the most-recently referred train
that is associated with Chicago can be referred to as {\em the Chicago train.}  If the user denies the system's assumption, the system considers alternative candidates.


\subsubsection{Implicature}
Each expression we utter can mean more than it literally means.  For
instance, {\em It is cold here} can mean such as  {\em Close the
window,} {\em Let's leave,} {\em Turn on the heater.}  One way to handle implicature is to assign a default interpretation to the expression.  The better way, however, is for the system to be able to reason about the context, and provide an interpretation that is most fitting for the context.


\subsubsection{Repair}
When false assumptions are made, it is important for the intelligent
system to be able to repair the mistake as we do in natural language
discourse.  In the implicature example above, the addressee B thinks
that the speaker A does not want to stay in the room, for example.
Then B may suggest them to leave the room, and B starts walking out
from the room.  If A's intention is for B to close the
window (if it is a cold, winter night and if the window is open) and
turn the heater on (if there is one), A has to stop B from leaving the
room, and clarify his intention of staying in the room.

Suppose there are two trains at Chicago (Northstar and Metroliner) in the TRAINs system, and the user utters the expression "the Chicago train" to mean Northstar.  Yet, if the system understands the same expression to refer to Metroliner because it happens to be the most-recently referred train that is associated with Chicago (see Assumptions above), the user can deny the system's infelicitous assumption.  Active logic acknowledges the mistake, and alternative candidates are considered; in this case, Northstar to be the referent of "the Chicago train."

When the discourse participants realize misunderstaiding (that is not necessarily due to false assumptions), repair will be made as well.  For example, B initially understands A's use of {\em Bush} to mean {\George Walker Bush}, the 43rd president of the United States, although A is talking about his father {\em George Herbert Walker Bush}, the 41st president.  A then tells B that {\em Bush} is the "read my lips" president.  Suppose B does not anything about the "read my lips" speech that the 41st president made, so B understand the 43rd president to be the "read my lips" president.  Later on when A tells B that {\em Bush} is married to Barbara, B realizes his misunderstanding.  At this point, B has to re-associate previous information with the appropriate referent; B then understands that the "read my lips" president is the 41st president.  

Another example is contradiction to the previous belief.  For example, if Tweety is a bird, active logic may believe that Tweety can fly based on an assumption that birds generally fly.  If it finds out later that Tweety is a penguin, contradiction arises because penguins do not fly.  In this case, active logic acknowledges that the present information (Penguins do not fly) is preferred over the previous belief (Birds generally fly) and makes an intelligent decision that Tweety does not fly.

\end{document}
